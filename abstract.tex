% The ability of Generative Adversarial Networks (GANs) to encode rich semantics within their latent space has been widely adopted for facial image editing. However, replicating their success with videos has proven challenging. Training video GANs is difficult, owing to a lack of high quality datasets. More crucially, videos introduce another dimension of realism that needs to be maintained - temporal consistency.
% When editing videos, the source is already coherent, and so the challenge in maintaining temporal realism is largely \textit{virtual}, arising in part due to careless treatment of individual components in the editing pipeline. We propose to leverage the natural alignment of StyleGAN-based models and the tendency of neural networks to learn low frequency functions, and demonstrate that they already provide a strongly consistent prior. We draw on these insights and propose a framework for semantic editing of faces in videos, demonstrating significant improvements over the current state-of-the-art, \ron{producing meaningful editing while retaining the temporal consistency. Unlike current works, we utilize challenging high quality talking heads videos with their original frame rate, showing our method applicable for arbitrary real videos. Code and videos will be published.}

Generative adversarial networks (GANs) have enabled photorealistic image synthesis and editing.
The design of such large-scale generators (e.g., StyleGAN2) is often based on heuristics or grid search methods that result in highly overparametrized networks.
Thus these models incur high computational costs, preventing their usage on resource-constrained devices.
In this work we propose a method for finding a compressed generator architecture which retains comparable performance.
Our main observation is that the architecutre search and model training phases can be decoupled.
This decoupling assures that an optimal architecutre would be missed during training, and simplifies the optimization process.
The accompanying code and results are available at \url{https://stitch-time.github.io/}
% The ability of Generative Adversarial Networks (GANs) to encode rich semantics within their latent space has been widely adopted for facial image editing. However, replicating their success with videos has proven challenging. Training video GANs is difficult, owing to a lack of high quality datasets. More crucially, videos introduce another dimension of realism that needs to be maintained - temporal consistency.
% When editing videos, the source is already coherent, and so the challenge in maintaining temporal realism is largely \textit{virtual}, arising in part due to careless treatment of individual components in the editing pipeline. We propose to leverage the natural alignment of StyleGAN-based models and the tendency of neural networks to learn low frequency functions, and demonstrate that they already provide a strongly consistent prior. We draw on these insights and propose a framework for semantic editing of faces in videos, demonstrating significant improvements over the current state-of-the-art, \ron{producing meaningful editing while retaining the temporal consistency. Unlike current works, we utilize challenging high quality talking heads videos with their original frame rate, showing our method applicable for arbitrary real videos. Code and videos will be published.}


The ability of Generative Adversarial Networks to encode rich semantics within their latent space has been widely adopted for facial image editing. However, replicating their success with videos has proven challenging. Sets of high-quality facial videos are lacking, and working with videos introduces a fundamental barrier to overcome --- \textit{temporal coherency}. We propose that this barrier is largely artificial. The source video is already temporally coherent, and deviations from this state arise in part due to careless treatment of individual components in the editing pipeline. We leverage the natural alignment of StyleGAN and the tendency of neural networks to learn low frequency functions, and demonstrate that they provide a strongly consistent prior. We draw on these insights and propose a framework for semantic editing of faces in videos, demonstrating significant improvements over the current state-of-the-art. Our method produces meaningful face manipulations, maintains a higher degree of temporal consistency, and can be applied to challenging, high quality, talking head videos which current methods struggle with. Our code and results are available at \url{https://stitch-time.github.io/}